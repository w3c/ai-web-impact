<!DOCTYPE html>
<html lang="en-us">
<head>
  <title>AI & the Web: Understanding and managing the impact of Machine Learning models on the Web</title>
  <meta charset=utf-8>
  <style>
    a[href].internalDFN {
	border-bottom-style: dotted;
    }
  </style>
  <script>var respecConfig = {
    darkMode: true,
    status: "UD",
    editors: [ {name: "Dominique Hazael-Massieux", email: "dom@w3.org"}],
    latestVersion: null,
    localBiblio: {
	"ISO/IEC-22989": {
	    href: "https://www.iso.org/standard/74296.html",
	    title: "Artificial intelligence concepts and terminology",
            "status": "Published",
            "publisher": "ISO/IEC",
            "isoNumber": "ISO 22989:2022",
            "date": "July 2022"
	},
	"C2PA-AI": {
	    href: "https://c2pa.org/specifications/specifications/1.3/ai-ml/ai_ml.html",
	    title: "Guidance for Artificial Intelligence and Machine Learning",
	    publisher: "C2PA"
	},
	"IPTC-DST": {
	    title: "Digital Source Type vocabulary",
	    href: "https://cv.iptc.org/newscodes/digitalsourcetype/",
	    publisher: "IPTC"
	},
	"UNESCO-AI": {
	    "href": "https://unesdoc.unesco.org/ark:/48223/pf0000380455",
	    "title": "Recommendation on the Ethics of Artificial Intelligence",
	    "publisher": "UNESCO",
	    "date": "2021"
	},
	"WAI-AI": {
	    title: "Artificial Intelligence (AI) and Accessibility Research Symposium 2023",
	    href: "https://www.w3.org/WAI/research/ai2023/",
	    publisher: "W3C Web Accessibility Initiative",
	    date: "Jan 2023"
	},
	"MODEL-CARDS": {
	    title: "Model Cards for Model Reporting",
	    href: "https://arxiv.org/pdf/1810.03993.pdf",
	    authors: ["M. Mitchell", "S. Wu", "A. Zaldivar", "P. Barnes", "L. Vasserman", "B. Hutchinson", "E. Spitzer", "I. Deborah Raji", "T. Gebru"],
	    date: "14 Juan 2019"
	},
	"W3C-ML-WS": {
	    "title": "W3C Workshop Report on Web and Machine Learning",
	    href: "https://www.w3.org/2020/06/machine-learning-workshop/report.html",
	    publisher: "W3C",
	    date: "October 2020"
	},
	"DIGITAL-CREDENTIALS": {
	    href: "https://wicg.github.io/digital-identities/",
	    title: "Digital Credentials",
	    publisher: "WICG",
	    date: "March 2024"
	},
	"TDMRep": {
	    href: "https://www.w3.org/community/reports/tdmrep/CG-FINAL-tdmrep-20240202/",
	    title: "Final Community Group Report",
	    date: "February 2024",
	    publisher: "Text and Data Mining Reservation Protocol Community Group"
	}
    },
    github: "https://github.com/w3c/ai-web-impact"
}</script>

  <script class="remove" src=
  "https://www.w3.org/Tools/respec/respec-w3c"></script>
</head>

<body>

<section id=abstract>
<p>
This document proposes an analysis of the systemic impact of [=AI systems=], and in particular ones based on [=Machine Learning models=], on the Web, and the role that Web standardization may play in managing that impact.
</p>
</section>
<section id=sotd>
<p>
This document is intended to capture the current shared understanding of the <a href="https://www.w3.org/staff/">W3C Team</a> on the current and expected impact of developments linked to [=Artificial Intelligence systems=] on the Web, and identifying explorations the World Wide Web Consortium  community has started or ought to be starting, to manage that impact. It does not represent any consensus from the W3C Membership nor is it a standardization document.
</p>
<p>
 The document was authored by Dominique Hazaël-Massieux (<a href="mailto:dom@w3.org">dom@w3.org</a>), with significant contributions from the rest of the W3C Team.</p>
<p>
This document aims first and foremost to help structure discussions on what may be needed at the standardization level to make the systemic impact of AI (and specifically, [=Machine Learning models=]) less harmful or more manageable. It is bound to be incomplete and sometimes wrong - we are gathering input and feedback in <a href="https://github.com/w3c/ai-web-impact/issues">GitHub</a>, preferably before June 30, 2024.
</p>
<p>
Depending on the feedback received, possible next steps include more in-depth stakeholder interviews, a dedicated W3C Workshop, or developing a standardization roadmap.</p>

</section>

<section>
  <h2>Executive Summary</h2>

  <p>[=Machine Learning models=] support a new generation of [=AI systems=]. These models are often [=training|trained=] on a large amount of Web content, deployed at scale through web interfaces, and can be used to generate plausible content at unprecedented speed and cost.</p>
  <p>Given the scope and scale of these intersections, this wave of [=AI systems=] is having potential systemic impact on the Web and some of the equilibriums on which its ecosystem had grown.</p>
  <p>This document reviews these intersections through their ethical, societal and technical impacts and highlights a number of areas where standardization, guidelines and interoperability could help manage these changes:</p>
  <ul>
    <li>a <a href="#b7">consent mechanism for the use of Web content in training pipelines</a>,</li>
    <li><a href="#b1">labeling content as computer-generated</a>,</li>
    <li><a href="#b2">surfacing training sources in model cards</a>,</li>
    <li><a href="#b3">exposing model-backed Web APIs</a>,</li>
    <li><a href="#b4">personal data stores</a> to reduce risk of private data exposure,</li>
    <li><a href="#b5">strengthening credentials and identity mechanisms</a> in light of new impersonation risks,</li>
    <li>an <a href="#b6">evalualation framework for the environmental impact of Web standards</a>,</li>
    <li>a <a href="#b8">framework to manage interoperability based on model inference</a>, including for non-deterministic models.</li>
  </ul>
  <p>We are <a href="https://github.com/w3c/ai-web-impact/issues">seeking input</a> from the community on proposals that could help make progress on these topics, and on other topics that this document has failed to identify.</p>
</section>

<section>
<h2>Introduction</h2>


<p>
Recent developments in the decades-long computer science field of Artificial Intelligence have made a number of systems emerge that already have started having <strong>systemic</strong> impacts on the Web and can be expected to further transform a number of shared expectations on which the health of the Web had relied so far.
</p>
<p>
To help structure a conversation within the W3C community (and possibly with other Web-related standards organization) about these transformations, this document collects the current shared understanding among the W3C Team of the intersections of "[=Artificial Intelligence=]", more specifically in the field of [=Machine Learning models=] (including Large Language Models and other so called generative AI models), with the Web as a system, and ongoing W3C development in this space. It is also a goal to raise questions about additional explorations that may be needed as further developments in this space arise.
</p>
<p>
That current understanding is bound to be incomplete or sometimes plain wrong; we hope that by publishing this document and inviting community reviews on it, we iteratively improve this shared understanding and help build a community roadmap to increase the positive impact and decrease the harms that are emerging in this intersection.
</p>
<section>
<h3>Terminology</h3>


<p>
The term "Artificial Intelligence" covers a very broad spectrum of algorithms, techniques and technologies. [[ISO/IEC-22989]] defines <dfn>Artificial Intelligence</dfn> as "research and development of mechanisms and applications of [=AI systems=]", with <dfn data-lt="Artificial Intelligence system">AI system</dfn>s being "an engineered system that generates outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives". At the time of the writing of this document in early 2024, the gist of the Web ecosystem conversation on Artificial Intelligence is mostly about systems based on <dfn>Machine Learning</dfn> ("process of optimizing model parameters through computational techniques, such that the model's behavior reflects the data or experience") and its software manifestation, <dfn data-lt="model">Machine Learning model</dfn>s ("mathematical construct that generates an inference or prediction based on input data or information").
</p>
<p>
While we acknowledge the much broader meaning of Artificial Intelligence and its intersection with a number of other Web- and W3C-related activities (e.g., the Semantic Web and Linked Data), this document willfully focuses only on the current conversation around the impact that these [=Machine Learning models=] are bringing to the Web. We further acknowledge that this document has been developed during, and is partially a response to, a cycle of inflated expectations and investments in that space.That situation underlines the need for a framework to structure the conversation.
</p>
<p>
Because of this focus on [=Machine Learning=], this document analyzes AI impact through the two main phases needed to operate [=Machine Learning models=]: <dfn>training</dfn> ("process to determine or to improve the parameters of a Machine Learning model, based on a Machine Learning algorithm, by using training data") and <dfn data-lt="run|running">inference</dfn> (the actual usage of these models to produce their expected outcomes), which we also casually refer as running a model.
</p>
</section>
</section>
<section>
<h2>Intersections between AI systems and the Web</h2>


<p>
A major role the Web plays is as a platform for content creators to expose at scale their content to content consumers. AI directly relates to these two sides of the platform:
</p>
<ul>

<li>In a number of cases, models are trained based on content crawled from the Web; the combination of scale and structure in that content (made possible by the underlying standards) has made it an invaluable source of training data that backs some of the most visible results in recent AI developments, such as large language models or image/video generators;

<li>Conversely, a number of these AI models can be used to generate content at unprecedented scale, which the reach of the Web allows to deploy seamlessly to the billions of users of the platform.
</li>
</ul>
<p>
When looking more specifically at the browser-mediated part of the Web which remains primarily a client/server architecture, AI models can be [=run=] either on the server-side or on the client-side (and somewhat more marginally at this point, in a <a href="https://github.com/webmachinelearning/proposals/issues/5">hybrid-fashion between the two</a>). On the client side, they can either be provided and operated by the browser (either at the user's request, or at the application's request), or entirely by the client-side application itself.
</p>
<p>
It's also worth noting that as [=AI systems=] are gaining rapid adoption, their intersection with the Web is bound to evolve and possibly trigger new systemic impact; for instance, emerging [=AI systems=] that combine [=Machine Learning models=] and content loaded from the Web in real-time may induce revisiting in depth the role and user experience of Web browsers in consuming or searching content.
</p>
</section>
<section>
<h2>Ethics and societal impact</h2>


<p>
The <a href="https://www.w3.org/TR/ethical-web-principles/">W3C's Technical Architecture Group Ethical Web Principles</a> [[ethical-web-principles]] includes ensuring "<a href="https://www.w3.org/TR/ethical-web-principles/#noharm">the Web should not cause harm to society</a>".
</p>
<p>
As described above, the Web is already a key enabler in some of the recent developments in Artificial Intelligence, and the usage and impact of Artificial Intelligence is being multiplied in scale through its distribution via the Web. This calls for the W3C community as stewards of the Web to understand potential harms emerging from that combination and to identify potential mitigations to these harms.
</p>
<p>
The <a href="https://www.w3.org/TR/webmachinelearning-ethics/">Ethical Principles for Web Machine Learning</a> [[webmachinelearning-ethics]] started in the <a href="https://www.w3.org/groups/wg/webmachinelearning">Web Machine Learning Working Group</a> combine values and principles from the UNESCO <a href="https://unesdoc.unesco.org/ark:/48223/pf0000380455">Recommendation on the Ethics of Artificial Intelligence</a> [[UNESCO-AI]] with Web-specific principles from the Ethical Web Principles to identify 4 values and 11 principles that integration of Machine Learning on the Web should follow, and which have helped structure this document.
</p>
<section>
<h3>Respecting <a href="https://www.w3.org/TR/webmachinelearning-ethics/#principle-3-autonomy">autonomy</a> and <a href="https://www.w3.org/TR/webmachinelearning-ethics/#principle-6-transparency-and-explainability">transparency</a></h3>
<section>

<h4>Transparency on AI-generated content</h4>


<p>
Recent [=AI systems=] are able to assist in the partial or complete creation of content (textual, graphic, audio and video) at a level of (at least superficially) credible quality and in quantities beyond that developed by humans. This provides both opportunities and risks for content creators, but more importantly, it creates a systemic risk for content consumers in no longer being able to distinguish or discover authoritative or curated content in a sea of credible (but either possibly or willfully wrong) generated content.
</p>
<p>
That need is pressing directly for end-users as they individually consume content, but also applies to agents that end-users rely on: typically, search engines would likely benefit from transparency on purely AI generated content. Somewhat ironically, crawlers used to train AI models are likely to need such a signal as well, since [=training=] models on the output of models may create unexpected and unwanted results.
</p>
<p>
We do not know of any credible solution that could guarantee (e.g., through cryptography) that a given piece of content was or was not generated (partially or entirely) by [=AI systems=]. That gap unfortunately leaves a systemic risk in terms of misinformation and spam that should be of grave concern for the health of the Web as a content distribution platform and of society as a whole.
</p>
<div id=b1 class=advisement>
<p>
A plausible role of standards in this space would be to at least facilitate the <strong>labeling of content</strong> to indicate whether it is the result of a <strong>computer-generated process</strong>. While such labels are unlikely to be enforceable through technical means, they could gain broad adoption with a combination of being automatically added by [=AI systems=] (possibly with enough friction that removing them would be at least somewhat costly at scale), and possibly serve as hooks in the regulatory context.
</p>
<p>
A number of proposals have already emerged in this space, which may benefit from more visibility, discussion and ultimately, scalable deployment:
</p>
<ul>

<li><a href="https://c2pa.org/specifications/specifications/1.3/ai-ml/ai_ml.html">C2PA Guidance for AI and Machine Learning</a> [[C2PA-AI]]

<li><a href="https://iptc.org/news/iptc-releases-draft-of-digital-source-type-vocabulary-to-support-synthetic-media/">IPTC synthetic media</a> [[IPTC-DST]] and its matching representation in <a href="https://github.com/schemaorg/schemaorg/issues/3392">Schema.org</a> [[schema-org]]

<li><a href="https://github.com/whatwg/html/issues/9479">Proposal: Meta Tag for AI Generated Content</a> in [[HTML]] (individual submission)
</li>
</ul>
</div>
</section>
<section>
<h4>Transparency on AI-mediated services</h4>


<p>
A well-known issue with relying operationally on [=Machine Learning models=] is that they will integrate and possibly strengthen any <a href="https://www.w3.org/TR/webmachinelearning-ethics/#bias">bias</a> in the data that was used during their [=training=]. Bias commonly occurs in other algorithms and human decision processes. Where [=AI systems=] make that bias a bigger challenge is because these models are at this point harder to audit and amend since they operate mostly as a closed box.
</p>
<p>
Such bias will disproportionately affect users whose expected input or output is less well represented in training data (as e.g., discussed in the <a href="https://www.w3.org/WAI/research/ai2023/">report from the 2023 AI & Accessibility research symposium</a> [[WAI-AI]]), which intuitively is likely to correlate strongly with users already disenfranchised by society and technology - e.g., if your language, appearance or behavior doesn't fit the mainstream-expected norm, you're less likely to feature in mainstream content and thus less visible or misrepresented in training data.
</p>
<p>
Until better tools emerge to facilitate at least the systematic detection of such bias, encouraging and facilitating the systematic publication of information on whether a Machine Learning model is in use, and how such a model was trained and checked for bias may help end- users make more informed choices about the services they use (which, of course, only helps if they have a choice in the first place, which may not apply e.g., to some government-provided services). 
</p>
<div id=b2 class=advisement>
<p>
<a href="https://arxiv.org/pdf/1810.03993.pdf">Model cards for Model Reporting</a> [[MODEL-CARDS]] are one of the approaches that were <a href="https://www.w3.org/2020/06/machine-learning-workshop/report.html#user">discussed in the 2020 W3C Workshop on Web and Machine Learning</a> [[W3C-ML-WS]]. Assuming this reporting provides meaningful and actionable transparency, a question for (the) technical standardization would be how such <strong>cards should be serialized and made discoverable on the Web</strong>.
</p>
</div>
<div id=b3 class=advisement>
<p>
W3C should look at a particular category of model deployments: <strong>models that are used by browser engines themselves to fulfill API</strong> requests. A number of Web browser APIs already expose (more or less explicitly) output of [=Machine Learning models=]:
</p>
<ul>

<li><a href="https://wicg.github.io/speech-api/">Web Speech API</a> [[SPEECH-API]]

<li><a href="https://wicg.github.io/shape-detection-api/">Accelerated Shape Detection API</a> [[SHAPE-DETECTION-API]]

<li><a href="https://w3c.github.io/mediacapture-extensions/#exposing-mediastreamtrack-source-background-blur-support">Background blur, face detection, gaze correction controls in Media capture</a>
</li>
</ul>
</div>
<p>
As described below, these APIs also raise engineering questions about how to ensure they provide the 

<a href="#interop">level of interoperability</a> that have been expected from more traditionally deterministic algorithms.
</p>
</section>
</section>

<section>
<h3>Right to <a href="https://www.w3.org/TR/webmachinelearning-ethics/#principle-4-right-to-privacy-and-data-protection">privacy and data control</a></h3>


<p>
Models trained on un-triaged or partially triaged content off the Web are bound to include personally identifiable information (PII). The same is true for models trained on data that users have chosen to share (for public consumption or not) with service providers. These models can often be made to retrieve and share that information with any user who knows how to ask, which breaks expectations of privacy for those whose personal information was collected, and is likely to be in breach with privacy regulations in a number of jurisdictions. Worse, they create risks for new types of attacks (see ‘Safety and security’).
</p>
<p>
  While the exclusion rules discussed in the context of content creation could partially help with the first situation, they would not help with the second one. This problem space is likely to be under heavy regulatory and legal scrutiny.</p>
  <p>From a technical standardization perspective, beyond labeling content, the emergence of user data being repurposed for model [=training=] and some of the pushback it is generating may bring renewed momentum (from user and service provider alike) behind decentralized architectures that leave user data under less centralized control (as illustrated by the recent widening adoption of Activity Streams).</p>
<div id=b4 class=advisement>
  <p>A particularly relevant instance of that pattern is emerging with so-called <strong>personal data stores</strong>: these provide ways for users to exert more fine-grained control of their data, by separating more clearly the roles of data store and data processor (which, in a traditional cloud infrastructure, would otherwise typically be handled by a single entity). 
</p>

<p>
That topic has most recently surfaced in W3C through the <a href="https://lists.w3.org/Archives/Public/public-new-work/2023Sep/0007.html">proposed charter for a SOLID Working Group</a> late 2023 (a charter that the W3C community has recognized as important, but where there is not yet consensus).
</p>
</div>

<p>
Allowing to [=run=] a model on personal data without uploading that data to a server is one of the key motivations behind the browser <a href="https://www.w3.org/TR/webnn/">Web Neural Network API</a> [[WEBNN]] which, completing the computing capabilities already provided by <a href="https://www.w3.org/groups/wg/wasm/">WebAssembly</a> [[WASM-CORE-2]] and <a href="https://www.w3.org/groups/wg/gpu/">WebGPU</a> [[WEBGPU]], provides additional Machine Learning specific optimizations to [=run=] models efficiently from within the browser (and thus, on the end-user device). 
</p>
</section>
<section>
<h3><a href="https://www.w3.org/TR/webmachinelearning-ethics/#principle-5-safety-and-security">Safety and security</a></h3>


<p>
A number of [=Machine Learning models=] have significantly lowered the cost of generating credible textual, as well as video (real-time or recorded) impersonations of real persons. This creates significant risks of upscaling the capabilities of phishing and other types of frauds, and thus raising much higher the barriers to establish trust in online interactions. If users no longer feel safe in their digitally-mediated interactions, the Web will no longer be able to play its role as a platform for these interactions.
</p>
<div id=b5 class=advisement>
<p>
This points towards even stronger needs for <strong>robust identity and credential management</strong> on the Web. The work of the <a href="https://www.w3.org/2017/vc/WG/">Verifiable Credentials Working Group</a> allows to express credentials in a cryptographically secure, privacy-preserving, and machine-verifiable way [[VC-DATA-MODEL]]. The <a href="https://lists.w3.org/Archives/Public/public-new-work/2024Jan/0006.html">proposed work to better integrate Federated Identity systems into browsers</a> [[FEDCM]] and the <a href="https://wicg.github.io/digital-identities/">emerging one on surfacing digital credentials to Web content</a> [[DIGITAL-CREDENTIALS]] are likely part of mitigating the risks associated with these new impersonation threats.
</p>
</div>
</section>
<section>
<h3><a href="https://www.w3.org/TR/webmachinelearning-ethics/#principle-8-sustainability">Sustainability</a></h3>


<p>
[=Training=] and [=running=] [=Machine Learning models=] can prove very resource-intensive, in particular in terms of power- and water-consumption. The imperative of reducing humanity's footprint on natural resources should apply particularly strongly to the technologies that standardization helps deploy at scale. 
</p>
<p>There is relatively new but promising work in the <a href="https://www.w3.org/community/sustyweb/">Sustainable Web Design Community Group</a> (a <a href="https://lists.w3.org/Archives/Public/public-new-work/2024Mar/0000.html">candidate to become a standardization Working Group</a>) to explain how to <a href="https://www.w3.org/blog/2023/introducing-web-sustainability-guidelines/">use Web technologies in a sustainable way</a>.</p>
<div id=b6 class=advisement>
<p>W3C still lacks a well-defined <strong>framework to evaluate the environmental impact of its standards</strong>. Given the documented high environmental impact of [=AI systems=], it will surely become more important that W3C groups that are expected to accelerate the deployment of [=Machine Learning models=] take a proactive approach in exploring and documenting how they envision the environmental impact of their work, and possible mitigations they might identify.</p>
</div>
</section>
<section>

<h3>Balancing content creators incentives and consumers rights</h3>


<p>
Some of the largest and most visible [=Machine Learning models=] are known or assumed to have been trained with materials crawled from the Web, without the explicit consent of their creators or publishers.
</p>
<p>
The controversy that has emerged from that situation is being debated (and in some cases, arbitrated) through the lens of copyright law.
</p>
<p>
It is not our place to determine if and how various copyright legislation bears on that particular usage. Beyond legal considerations, the copyright system creates a (relatively) shared understanding between creators and consumers that, by default, content cannot be redistributed, remixed, adapted or built upon without creators consent. This shared understanding made it possible for a lot of content to be openly distributed on the Web. It also allowed creators to consider a variety of monetization options (subscription, pay per view, advertising) for their content grounded on the assumption that consumers will always reach their pages.
</p>
<p>
A number of [=AI systems=] combine (1) automated large-scale consumption of Web content, and (2) production at scale of content, in ways that do not recognize or otherwise compensate content it was trained from.
</p>
<p>
 
</p>
<p>
While some of these tensions are not new (as discussed below), Machine Learning based systems are poised to upend the existing balance. Unless a new sustainable equilibrium is found, this exposes the Web to the following undesirable outcomes:
</p>
<ul>

<li>Significantly less open distributed content (which would likely have a disproportionate impact on the less wealthy part of the population)

<li>A less appealing platform to distribute content
</li>
</ul>
<p>
A less direct risk may emerge from changes in copyright laws meant to help to rebalance the situation but which would reduce the rights from content consumers and then undermine the value of the Web as a platform for which content distribution is a key value proposition.
</p>
<section>
<h4>Comparison with search engines</h4>


<p>
A number of the tensions emerging around the re-use of content crawled at scale from the Web have a long history given the central role that search engines play for the Web. Indeed, search engines provide (and absorb) value from their ability to retrieve and organize information from content on the Web, and they heavily rely on the standardized infrastructure this content is built on to achieve these results.
</p>
<p>
The more or less implicit contract that emerged between search engines and content providers has been that search engines can retrieve, parse and partially display content from the providers, in exchange of bringing more visibility and traffic to them. A further assumption has been encoded in the way the Web operates that this contract is the default for anyone making content available publicly on the Web, with an opt-out mechanism encoded via the <a href="https://www.rfc-editor.org/rfc/rfc9309.html"><code>robots.txt</code> directives</a> [[RFC9309]].
</p>
<p>
Over time, in addition to the links to sites matching the user's query, search engines have integrated more ways to surface content directly from the target Web sites: either through rich snippets (typically made possible by the use of schema.org metadata) or through embedded preview (e.g., what the <a href="https://amp.dev/">AMP project</a> enabled). These changes were frequently accompanied by sometimes challenging discussions around the balance between bringing additional visibility to crawled content and reducing the incentive from end-users to visit the source website (e.g., because they may have already received sufficient information from the search results page).
</p>
<p>
In a certain number of cases, [=AI systems=] are used as an alternative or complement to what users would traditionally have used a search engine for (and indeed, are increasingly integrated into search engines interfaces). So it seems useful to explore to what extent the lessons learned from the evolutionary process balancing the needs from search engines and from content creators can inform the discussion on crawlers used to train [=Machine Learning models=].
</p>
<p>
In making that comparison, it's also important to note significant differences:
</p>
<ul>

<li>The implicit contract that content creators expect from search engines crawlers –i.e., that they will bring exposure to their content– does not have a systematic equivalent for content integrated into [=AI systems=]; while some such systems are gaining the ability to point back to the source of their training data used in a given [=inference=], this is hardly a widespread feature of these systems, nor is it obvious it could be applied systematically (e.g., would linking back to sources for a generated image even make sense?)

<li><code>robots.txt</code> directives allow to give specific rules to specific crawlers based on their user agent; while this has been practically manageable when dealing with (for better or for worse) few well-known search engine crawlers, expecting content creators to maintain potential allow- and block-list of the rapidly expanding number of crawlers deployed to retrieve training data seems unlikely to achieve sustainable results
</li>
</ul>
<p>
Given the likely different expectations around the quid-pro-quo of crawling in the context of [=AI systems=], it is not obvious that the permission-less pattern inherited from the early days of the Web (robots.txt was designed in 1994) would be a satisfactory match to ensure the long term sustainability of content publication on the Web (itself presumably in the long term interest of AI crawlers themselves).
</p>
<div id=b7 class=advisement>
<p>
In general, a possibly helpful line of inquiry for standards in this space would be to identify solutions that help <strong>content producers and AI crawlers to find agreeable terms, ideally at a scale</strong> that would make it appealing to all parties.
</p>
<p>
Several groups and individuals have been exploring how to make it possible for content publishers to express their willingness to get their content used for [=training=] [=Machine Learning models=]:
</p>
<ul>

<li>The <a href="https://www.w3.org/groups/cg/tdmrep">Text and Data Mining Reservation Protocol Community Group</a> has developed the <a href="https://www.w3.org/community/reports/tdmrep/CG-FINAL-tdmrep-20240202/">TDM Reservation Protocol (TDMRep)</a> [[TDMRep]] to "express the reservation of rights relative to text & data mining applied to lawfully accessible Web content"

<li>Discussions have started in IETF around updating the robots.txt directives; among others, the <a href="https://www.w3.org/community/robotstxt/">update robots.txt Community Group</a> proposes to add an opt-in mechanism to the robots.txt directives.
</li>
</ul>
</div>
</section>
</section>
</section>
<section>
<h2 id=interop>Impact of [=AI systems=] on interoperability</h2>


<p>
  A key part of <a href="https://www.w3.org/TR/w3c-vision/#vision-web">W3C's vision for the Web</a> [[w3c-vision]] is to ensure the Web is developed around principles of interoperability: that is, for technologies that W3C codifies as Web standards, to ensure they are implemented and deployed in a way that will work the same across products, allowing for greater choice for users and fostering the long term viability of the content.</p>
<p>When the algorithm on which interoperability relies is deterministic, ensuring interoperability is a matter of describing in sufficient detail and clarity the said algorithm, and running sufficient testing on the products to verify they achieve the intended result. The move to more <a href="https://www.w3.org/TR/design-principles/#algorithms">algorithmic specifications</a> [[design-principles]] and thorough automated testing (e.g., through the <a href="https://web-platform-tests.org/">Web Platform Tests project</a>) has largely been driven by the goal of providing a robust interoperable platform.
</p>
<div id=b8 class=advisement>
<p>
As discussed above, [=Machine Learning models=] are already finding their ways in standardized Web APIs. These creates two challenges to our interoperability goals:
</p>
<ul>

<li>[=Machine Learning models=] are mostly not built or described as a series of algorithmic steps. If a given standardized behavior is expected to be best fulfilled by [=Machine Learning models=], how should that behavior be specified? How can it be tested to a level that sufficiently verifies an <strong>interoperable outcome across products that would use different models</strong>? What impact would it have on the fingerprinting surface of the browsers?

<li>A number of important [=Machine Learning models=] are not deterministic; if or when some of these <strong>non-deterministic models</strong> get exposed in standardized APIs, this consistency question is no longer limited to two products using two different models, since a given input would no longer produce a predetermined output. It is not clear to us at the moment how to prepare for interoperable behaviors based on non-deterministic models, which probably raises the question of whether and how such models should be acceptable as part of interoperable implementations.
</li>
</ul>
</div>
<p>
A possible consequence of these challenges is a reduction of the scope of what can be meaningfully made interoperable and standardized as a possibly growing number of features get mediated by [=Machine Learning models=] (similar to the <a href="https://datatracker.ietf.org/doc/html/draft-tschofenig-post-standardization-02">postulated impact of growing capabilities of web applications on the need to standardize protocols</a>). In that context, discussions e.g., around AI-based codecs point towards possible significant changes in the interoperability landscape.
</p>
</section>
</body>
</html>
